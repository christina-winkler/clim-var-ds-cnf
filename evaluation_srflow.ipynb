{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80b4c112",
   "metadata": {},
   "source": [
    "#  Evaluating Performance of Trained Conditioned Normalizing Flows for Climate Variable Downscaling\n",
    "In this notebook, we will evaluate the performance of trained conditioned normalizing flows (CNFs) for downscaling climate variables. Conditioned normalizing flows are a powerful class of generative models that learn to map a simple distribution (e.g., Gaussian) to a complex target distribution by conditioning on additional information, such as low-resolution climate data. The main objective of this evaluation is to assess how well the trained CNF model can downscale climate variables, such as temperature, precipitation, or humidity, from coarse-resolution to fine-resolution spatial grids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50932f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from models.architectures import srgan, srflow\n",
    "import PIL\n",
    "import os\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import pandas as pd\n",
    "\n",
    "# Dataset loading\n",
    "from data import dataloading\n",
    "from data.era5_temp_dataset import InverseMinMaxScaler\n",
    "\n",
    "from os.path import exists, join\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import transforms\n",
    "import timeit\n",
    "import pdb\n",
    "import argparse\n",
    "import seaborn as sns\n",
    "\n",
    "from utils.metrics import *\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba156bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GPUs avail:', torch.cuda.device_count())\n",
    "\n",
    "# Parse Settings\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# train configs\n",
    "parser.add_argument(\"--model\", type=str, default=\"srflow\",\n",
    "                    help=\"Model you want to train.\")\n",
    "parser.add_argument(\"--modeltype\", type=str, default=\"srflow\",\n",
    "                    help=\"Specify modeltype you would like to train [srflow, cdiff, srgan].\")\n",
    "parser.add_argument(\"--model_path\", type=str, default=\"runs/\",\n",
    "                    help=\"Directory where models are saved.\")\n",
    "parser.add_argument(\"--modelname\", type=str, default=None,\n",
    "                    help=\"Sepcify modelname to be tested.\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=10000,\n",
    "                    help=\"number of epochs\")\n",
    "parser.add_argument(\"--max_steps\", type=int, default=2000000,\n",
    "                    help=\"For training on a large dataset.\")\n",
    "parser.add_argument(\"--log_interval\", type=int, default=100,\n",
    "                    help=\"Interval in which results should be logged.\")\n",
    "\n",
    "# runtime configs\n",
    "parser.add_argument(\"--visual\", action=\"store_true\",\n",
    "                    help=\"Visualizing the samples at test time.\")\n",
    "parser.add_argument(\"--noscaletest\", action=\"store_true\",\n",
    "                    help=\"Disable scale in coupling layers only at test time.\")\n",
    "parser.add_argument(\"--noscale\", action=\"store_true\",\n",
    "                    help=\"Disable scale in coupling layers.\")\n",
    "parser.add_argument(\"--testmode\", action=\"store_true\",\n",
    "                    help=\"Model run on test set.\")\n",
    "parser.add_argument(\"--train\", action=\"store_true\",\n",
    "                    help=\"If model should be trained.\")\n",
    "parser.add_argument(\"--resume_training\", action=\"store_true\",\n",
    "                    help=\"If training should be resumed.\")\n",
    "parser.add_argument(\"--constraint\", type=str, default='scaddDS',\n",
    "                    help=\"Physical Constraint to be applied during training.\")                   \n",
    "\n",
    "# hyperparameters\n",
    "parser.add_argument(\"--nbits\", type=int, default=8,\n",
    "                    help=\"Images converted to n-bit representations.\")\n",
    "parser.add_argument(\"--s\", type=int, default=16, help=\"Upscaling factor.\")\n",
    "parser.add_argument(\"--bsz\", type=int, default=16, help=\"batch size\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"learning rate\")\n",
    "parser.add_argument(\"--filter_size\", type=int, default=512, help=\"filter size NN in Affine Coupling Layer\")\n",
    "parser.add_argument(\"--L\", type=int, default=3, help=\"# of levels\")\n",
    "parser.add_argument(\"--K\", type=int, default=2,\n",
    "                    help=\"# of flow steps, i.e. model depth\")\n",
    "parser.add_argument(\"--nb\", type=int, default=16,\n",
    "                    help=\"# of residual-in-residual blocks LR network.\")\n",
    "parser.add_argument(\"--condch\", type=int, default=128//8,\n",
    "                    help=\"# of residual-in-residual blocks in LR network.\")\n",
    "\n",
    "# data\n",
    "parser.add_argument(\"--datadir\", type=str, default=\"/home/christina/Documents/clim-var-ds-cnf-own/data/\",\n",
    "                    help=\"Dataset to train the model on.\")\n",
    "parser.add_argument(\"--trainset\", type=str, default=\"era5-TCW\", help='[era5-TCW, era5-T2M]')\n",
    "parser.add_argument(\"--testset\", type=str, default=\"era5-TCW\", help=\"Specify test dataset\")\n",
    "\n",
    "args = parser.parse_args('')\n",
    "config = vars(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22aa07d",
   "metadata": {},
   "source": [
    "### Load ERA5 Watercontent Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be374a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testset\n",
    "train_loader,val_loader, test_loader, args = dataloading.load_data(args)\n",
    "\n",
    "in_channels = next(iter(test_loader))[0].shape[1]\n",
    "height, width = next(iter(test_loader))[0].shape[2], next(iter(test_loader))[0].shape[3]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    args.device = torch.device(\"cuda\")\n",
    "    args.num_gpus = torch.cuda.device_count()\n",
    "    args.parallel = False\n",
    "\n",
    "else:\n",
    "    args.device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fbeedd",
   "metadata": {},
   "source": [
    "### Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03be41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to accumulate pixel values for each channel\n",
    "all_unnorm_pixels = {0: []}  # Adjust the dictionary based on the number of channels\n",
    "\n",
    "for idx, item in enumerate(train_loader):\n",
    "    images = item[2]\n",
    "    if idx == 20:\n",
    "        break\n",
    "    for channel in range(images.size(1)):\n",
    "        all_unnorm_pixels[channel].append(images[:, channel, :, :].view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36db46a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all pixel values for each channel\n",
    "for channel in all_unnorm_pixels:\n",
    "    all_unnorm_pixels[channel] = torch.cat(all_unnorm_pixels[channel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5659fd7b-9a40-48fa-8ae6-cdc2b1c7bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute min max std mean median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece1d136-08a2-4f58-afe2-86c9bd594df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary values to a list of lists (assuming same number of pixels per sample)\n",
    "pixel_list = list(all_unnorm_pixels.values())\n",
    "\n",
    "# Convert to a PyTorch tensor\n",
    "all_unnorm_pixels_tensor = pixel_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06422b4-ba01-4ddd-b149-a46eb18ebfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics\n",
    "min_value = torch.min(all_unnorm_pixels_tensor).item()\n",
    "max_value = torch.max(all_unnorm_pixels_tensor).item()\n",
    "mean_value = torch.mean(all_unnorm_pixels_tensor).item()\n",
    "std_deviation = torch.std(all_unnorm_pixels_tensor).item()\n",
    "median_value = torch.median(all_unnorm_pixels_tensor.float()).item()  # Median requires float tensor\n",
    "\n",
    "# Compute variance using PyTorch\n",
    "variance = torch.var(all_unnorm_pixels_tensor.float()).item()\n",
    "\n",
    "# Compute percentiles using NumPy (as PyTorch does not have built-in percentile function)\n",
    "percentiles = np.percentile(all_unnorm_pixels_tensor.numpy(), [25, 50, 75])  # Change the percentiles as needed\n",
    "\n",
    "# Print the results\n",
    "print(f\"Minimum pixel value: {min_value}\")\n",
    "print(f\"Maximum pixel value: {max_value}\")\n",
    "print(f\"Mean pixel value: {mean_value}\")\n",
    "print(f\"Standard deviation of pixel values: {std_deviation}\")\n",
    "print(f\"Median pixel value: {median_value}\")\n",
    "print(f\"Variance of pixel values: {variance}\")\n",
    "print(f\"25th, 50th (median), and 75th percentiles: {percentiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a6abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the tensor to 1D for boxplot\n",
    "all_unnorm_pixels_flat = all_unnorm_pixels_tensor.view(-1).numpy()\n",
    "\n",
    "# Plot histogram of pixel values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(all_unnorm_pixels_flat, bins=124, color='blue', alpha=0.7)\n",
    "plt.title('Unnormalized Pixel Value Distribution - ERA5 watercontent')\n",
    "plt.xlabel('Pixel Intensity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.savefig('./data-analysis/unnorm_pix_val_distr_watercontent___.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde509a4-cfcf-4e70-9e54-fdf663d87d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a boxplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(all_unnorm_pixels_flat, vert=False, patch_artist=True, meanline=True, showmeans=True)\n",
    "plt.title('Boxplot of Pixel Values - ERA5 Watercontent')\n",
    "plt.xlabel('Pixel Values')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot if needed\n",
    "plt.savefig('./data-analysis/pixel_values_watercontent_boxplot.png')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df3f06e-73a1-4947-a393-488799805a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(all_unnorm_pixels_tensor)\n",
    "top_values, top_indices = torch.topk(all_unnorm_pixels_tensor, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76e140d-541c-482c-aa88-ca44f325ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_values, top_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f890ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use min-max due to non-gaussian distribution of data and outlier handling\n",
    "def min_max_scaler(x, min_val=0, max_val=124):\n",
    "    if min_val is None:\n",
    "        min_val = x.min()\n",
    "    if max_val is None:\n",
    "        max_val = x.max()\n",
    "    scaled_x = (x - min_val) / (max_val - min_val)\n",
    "    return scaled_x, min_val, max_val\n",
    "\n",
    "def inv_min_max_scaler(scaled_x, min_val=0, max_val=124):\n",
    "    x = scaled_x * (max_val - min_val) + min_val\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b100ec-0042-43f2-89ef-ad34e897ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661b556b-7520-4fdc-9f5a-71ce975399b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-max scaler example\n",
    "scaled_data, min_val, max_val = min_max_scaler(images, min_val=0, max_val=124)\n",
    "# print(\"Scaled data (min-max scaler):\", scaled_data)\n",
    "original_data = inv_min_max_scaler(scaled_data, min_val=0, max_val=124)\n",
    "# print(\"Original data (min-max scaler):\", original_data)\n",
    "print((images-original_data).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bda1c2-a12b-4022-9026-fa697ecd7624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, title='', save_path=None, figsize=(15, 10)):\n",
    "    num_images = images.shape[0]\n",
    "    rows = int(num_images ** 0.5)  # Square root of number of images for rows\n",
    "    cols = (num_images + rows - 1) // rows  # Ensure enough columns\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(num_images):\n",
    "        image = images[i].squeeze().numpy()  # Squeeze to remove single channel dimension\n",
    "        axes[i].imshow(image, cmap='viridis')  # Plot grayscale image with viridis colormap\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Set title\n",
    "    fig.suptitle(title, fontsize=16, y=1.02)\n",
    "\n",
    "    # Hide any remaining axes\n",
    "    for j in range(num_images, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save figure if save_path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plot the images from the tensor with title and save the figure\n",
    "plot_images(images, title='ERA5 Watercontent High-Resolution Samples', save_path='./data-analysis/pixel_values_watercontent_samples.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf939c9d-22c0-4375-9371-97140de7e858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7accbe0-12b1-453c-ae19-3eb1b14fdb99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc9a1de5-803e-42ac-9f17-d68fee26e970",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "044d11cf-4a99-4c46-aa93-796f75884707",
   "metadata": {},
   "source": [
    "### Metric Evaluation\n",
    "Evaluations: \n",
    "- CNF unconstrained vs. constrained\n",
    "- CNF vs. GAN vs. CDiff\n",
    "- Residual Error Plots for ???\n",
    "- Metrics for 2x,4x,8x,16x with and without constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9ac8cb-3138-48a9-a8c6-143c6c1078c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from utils import metrics\n",
    "\n",
    "def save_snapshot(tensor, path):\n",
    "    # Function to save tensor as image in Viridis color scheme\n",
    "    plt.figure()\n",
    "    plt.imshow(tensor[0,...].cpu().numpy().squeeze(), cmap='viridis')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "def save_grid_images(tensor_batch, save_path, nrow=3, title=None):\n",
    "    \n",
    "    # Create a grid of images\n",
    "    grid_img = vutils.make_grid(tensor_batch[0:9], nrow=nrow, normalize=True, scale_each=True)\n",
    "\n",
    "    # Convert the grid to numpy and plot\n",
    "    np_grid = grid_img.cpu().numpy().transpose((1, 2, 0))\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(np_grid[:, :, 0], cmap='viridis')  # Use Viridis colormap for grayscale images\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout(pad=0)\n",
    "\n",
    "    # Save the grid image\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "def metric_eval(dataloader, model, exp_name, args):\n",
    "    metric_dict = {\n",
    "        'rmse0': [], 'mse0': [], 'mae0': [],\n",
    "        'rmse05': [], 'mse05': [], 'mae05': [],\n",
    "        'rmse08': [], 'mse08': [], 'mae08': [],\n",
    "        'rmse1': [], 'mse1': [], 'mae1': [],\n",
    "        'crps': []\n",
    "    }\n",
    "\n",
    "    # Create directories\n",
    "    results_dir = os.path.join(exp_name, 'experiment_results')\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    snapshots_dir = os.path.join(results_dir, 'snapshots')\n",
    "    os.makedirs(snapshots_dir, exist_ok=True)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, item in enumerate(dataloader):\n",
    "            y = item[0].to(args.device)\n",
    "            x = item[1].to(args.device)\n",
    "\n",
    "            y_unorm = item[2].to(args.device)\n",
    "            x_unorm = item[3].to(args.device)\n",
    "\n",
    "            z, _ = model.forward(x_hr=y, xlr=x)\n",
    "\n",
    "            # Evaluate for different temperatures\n",
    "            mu0, _, _ = model(xlr=x, reverse=True, eps=0)\n",
    "            mu05, _, _ = model(xlr=x, reverse=True, eps=0.5)\n",
    "            mu08, _, _ = model(xlr=x, reverse=True, eps=0.8)\n",
    "            mu1, _, _ = model(xlr=x, reverse=True, eps=1.0)\n",
    "\n",
    "            # Compute and store MSE for each temperature\n",
    "            mse0 = metrics.MSE(inv_min_max_scaler(mu0), y_unorm).detach().cpu().numpy()\n",
    "            mse05 = metrics.MSE(inv_min_max_scaler(mu05), y_unorm).detach().cpu().numpy()\n",
    "            mse08 = metrics.MSE(inv_min_max_scaler(mu08), y_unorm).detach().cpu().numpy()\n",
    "            mse1 = metrics.MSE(inv_min_max_scaler(mu1), y_unorm).detach().cpu().numpy()\n",
    "            \n",
    "            metric_dict['mse0'].append(mse0)\n",
    "            metric_dict['mse05'].append(mse05)\n",
    "            metric_dict['mse08'].append(mse08)\n",
    "            metric_dict['mse1'].append(mse1)\n",
    "\n",
    "            # Compute and store MAE for each temperature\n",
    "            mae0 = metrics.MAE(inv_min_max_scaler(mu0), y_unorm).detach().cpu().numpy()\n",
    "            mae05 = metrics.MAE(inv_min_max_scaler(mu05), y_unorm).detach().cpu().numpy()\n",
    "            mae08 = metrics.MAE(inv_min_max_scaler(mu08), y_unorm).detach().cpu().numpy()\n",
    "            mae1 = metrics.MAE(inv_min_max_scaler(mu1), y_unorm).detach().cpu().numpy()\n",
    "\n",
    "            metric_dict['mae0'].append(mae0)\n",
    "            metric_dict['mae05'].append(mae05)\n",
    "            metric_dict['mae08'].append(mae08)\n",
    "            metric_dict['mae1'].append(mae1)\n",
    "\n",
    "            # Compute and store RMSE for each temperature\n",
    "            rmse0 = metrics.RMSE(inv_min_max_scaler(mu0), y_unorm).detach().cpu().numpy()\n",
    "            rmse05 = metrics.RMSE(inv_min_max_scaler(mu05), y_unorm).detach().cpu().numpy()\n",
    "            rmse08 = metrics.RMSE(inv_min_max_scaler(mu08), y_unorm).detach().cpu().numpy()\n",
    "            rmse1 = metrics.RMSE(inv_min_max_scaler(mu1), y_unorm).detach().cpu().numpy()\n",
    "\n",
    "            metric_dict['rmse0'].append(rmse0)\n",
    "            metric_dict['rmse05'].append(rmse05)\n",
    "            metric_dict['rmse08'].append(rmse08)\n",
    "            metric_dict['rmse1'].append(rmse1)\n",
    "\n",
    "            # Save grid of images for visualization (adjust as needed)\n",
    "            if batch_idx == 0:  # Save only for the first batch for simplicity\n",
    "                # Save snapshots\n",
    "                save_grid_images(y_unorm, os.path.join(snapshots_dir, f'ground_truth_{batch_idx}.png'))\n",
    "                save_grid_images(mu0, os.path.join(snapshots_dir, f'prediction_mu0_{batch_idx}.png'))\n",
    "                save_grid_images(mu05, os.path.join(snapshots_dir, f'prediction_mu05_{batch_idx}.png'))\n",
    "                save_grid_images(mu08, os.path.join(snapshots_dir, f'prediction_mu08_{batch_idx}.png'))\n",
    "                save_grid_images(mu1, os.path.join(snapshots_dir, f'prediction_mu1_{batch_idx}.png'))\n",
    "                save_grid_images(x_unorm, os.path.join(snapshots_dir, f'low_res_{batch_idx}.png'))\n",
    "\n",
    "            # Calculate CRPS for ensemble\n",
    "            crps_stack = torch.stack([mu0, mu05, mu08, mu1], dim=1)\n",
    "            metric_dict['crps'].append(crps_ensemble(y_unorm, crps_stack))\n",
    "            \n",
    "            print(f'Current RMSE - mu0: {rmse0}, mu05: {rmse05}, mu08: {rmse08}, mu1: {rmse1}')\n",
    "            print(f'Current MAE - mu0: {mae0}, mu05: {mae05}, mu08: {mae08}, mu1: {mae1}')\n",
    "            print(f'Current CRPS:{ crps_ensemble(y_unorm, crps_stack).mean()}')\n",
    "\n",
    "            if batch_idx == 5:\n",
    "                break\n",
    "\n",
    "    # Create a string representation of the metric_dict\n",
    "    mean_dict = {}\n",
    "    for key, value in metric_dict.items():\n",
    "        if len(value) > 0:\n",
    "            mean_dict[key] = np.mean(value)   \n",
    "            \n",
    "    metric_str = \"\\n\".join([f\"{key}: {value}\" for key, value in mean_dict.items()])\n",
    "\n",
    "    # Save metric_dict to a text file\n",
    "    with open(os.path.join(results_dir, 'metrics.txt'), 'w') as f:\n",
    "        json.dump(metric_str, f, indent=4)\n",
    "        \n",
    "    print(metric_dict)\n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c4e6b-6b1a-487b-879d-ad13fa25eb7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "438d3f4e-444e-4910-bb9a-3bdc3042b30e",
   "metadata": {},
   "source": [
    "### Bicubic Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9b72ea-2e2f-4476-9946-48fc8379f0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bicubic_interpolation(x_lr, scale_factor):\n",
    "    # Implement bicubic interpolation here using torchvision.transforms.functional.resize\n",
    "    # Example: Replace this with your actual bicubic interpolation function\n",
    "    x_hr = torch.nn.functional.interpolate(x_lr, scale_factor=scale_factor, mode='bicubic', align_corners=False)\n",
    "    return x_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d2e3fd-1557-4e6b-ace2-fb277f84ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bicubic(dataloader, exp_name='bicubic-interpolation', scale=2.0):\n",
    "    metric_dict = {'rmse': [], 'mse': [], 'mae': []}\n",
    "\n",
    "    # Create directories\n",
    "    results_dir = os.path.join(exp_name, 'experiment_results')\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    snapshots_dir = os.path.join(results_dir, 'snapshots')\n",
    "    os.makedirs(snapshots_dir, exist_ok=True)\n",
    "\n",
    "    args.device = 'cuda'\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, item in enumerate(dataloader):\n",
    "            \n",
    "            y = item[0].to(args.device)\n",
    "            x = item[1].to(args.device)\n",
    "\n",
    "            y_unorm = item[2].to(args.device)\n",
    "            x_unorm = item[3].to(args.device)\n",
    "\n",
    "            # Perform bicubic interpolation for each scale factor\n",
    "            mu0 = bicubic_interpolation(x, scale_factor=scale)\n",
    "\n",
    "            # Compute and store MSE for each temperature\n",
    "            mse = metrics.MSE(inv_min_max_scaler(mu0), y_unorm).detach().cpu().numpy()\n",
    "            metric_dict['mse'].append(mse)\n",
    "        \n",
    "            mae = metrics.MAE(inv_min_max_scaler(mu0), y_unorm).detach().cpu().numpy()\n",
    "            metric_dict['mae'].append(mae)\n",
    "\n",
    "            # Compute and store RMSE for each temperature\n",
    "            rmse = metrics.RMSE(inv_min_max_scaler(mu0), y_unorm).detach().cpu().numpy()\n",
    "            metric_dict['rmse'].append(rmse)\n",
    "\n",
    "            # Save grid of images for visualization (adjust as needed)\n",
    "            if batch_idx == 0:  # Save only for the first batch for simplicity\n",
    "                # Save snapshots\n",
    "                save_grid_images(y_unorm, os.path.join(snapshots_dir, f'ground_truth_{batch_idx}.png'))\n",
    "                save_grid_images(mu0, os.path.join(snapshots_dir, f'prediction_mu0_{batch_idx}.png'))\n",
    "                save_grid_images(x_unorm, os.path.join(snapshots_dir, f'low_res_{batch_idx}.png'))\n",
    "                \n",
    "        # Create a string representation of the metric_dict\n",
    "        mean_dict = {}\n",
    "        for key, value in metric_dict.items():\n",
    "            if len(value) > 0:\n",
    "                mean_dict[key] = np.mean(value)   \n",
    "                \n",
    "        metric_str = \"\\n\".join([f\"{key}: {value}\" for key, value in mean_dict.items()])\n",
    "    \n",
    "        # Save metric_dict to a text file\n",
    "        with open(os.path.join(results_dir, 'metrics.txt'), 'w') as f:\n",
    "            json.dump(metric_str, f, indent=4)\n",
    "        \n",
    "        return mean_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb2b633-3e68-4557-97da-971aaf9544e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.s = 2\n",
    "# train_loader,val_loader, test_loader, args = dataloading.load_data(args)\n",
    "# eval_bicubic(test_loader, exp_name='./runs/bicubic-interpolation-2', scale=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428eefb4-211b-48b8-a93c-2e11210c72f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.s = 4\n",
    "# train_loader,val_loader, test_loader, args = dataloading.load_data(args)\n",
    "# eval_bicubic(test_loader, exp_name='./runs/bicubic-interpolation-4', scale=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96efcfde-39a1-4ad5-8d66-bf32697ff949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.s = 8\n",
    "# train_loader,val_loader, test_loader, args = dataloading.load_data(args)\n",
    "# eval_bicubic(test_loader, exp_name='./runs/bicubic-interpolation-8', scale=8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c976bf7-3aba-4c50-86cd-f7e06bf23040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.s = 16\n",
    "# train_loader,val_loader, test_loader, args = dataloading.load_data(args)\n",
    "# eval_bicubic(test_loader, exp_name='./runs/bicubic-interpolation-16', scale=16.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0296847-fcfc-469a-a07d-791bbfce970f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea1b371-4a0e-493b-a8b5-7382d2f377e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b53feb-55e6-4d87-981a-82a161041229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d384d-04ef-4c3b-83e7-a07f653ff0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2319b35d-3232-4656-b34c-c1f74fff60b9",
   "metadata": {},
   "source": [
    "# Experiments: Comparing across upsampling factors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95618d-f5be-4b6f-a9f6-a0a2a4ebcc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first experiment: load 2x model for CNF with each constraint and without. See if the predictions improve when having the constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94723a01-a2eb-42ec-99c3-cea05ba8a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot unnormalized pixel value distributions!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec047c-0bcd-4a5d-addc-75349b0ed27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the effect of epsilon sampling parameter on extreme value prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2789e1c-d216-4435-af99-aeca9aa03344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train flow with Laplace prior quickly to show advantage of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a4b614-e1c0-4cbd-89a8-ed196ff4dfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as D\n",
    "\n",
    "class LaplacePrior:\n",
    "    def __init__(self, mu, b):\n",
    "        self.mu = mu  # Mean parameter\n",
    "        self.b = b    # Scale parameter\n",
    "        \n",
    "    def log_prob(self, x):\n",
    "        dist = D.Laplace(self.mu, self.b)\n",
    "        return dist.log_prob(x)\n",
    "\n",
    "# Example usage:\n",
    "mu = torch.tensor(0.0)  # Mean parameter\n",
    "b = torch.tensor(1.0)   # Scale parameter\n",
    "\n",
    "prior = LaplacePrior(mu, b)\n",
    "\n",
    "# Calculate log probability for a sample x\n",
    "x = torch.tensor(0.5)   # Sample value\n",
    "log_prob = prior.log_prob(x)\n",
    "\n",
    "print(f\"Log probability of x={x}: {log_prob.item()}\")  # Convert to float for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a0cf2a-22ea-4f14-9e84-3911f1974901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "print('Num of avail GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082d2d8-ec1f-475f-8b15-b2daeb670580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def load_model(exp_dir, model, mpath):\n",
    "    \"\"\"\n",
    "    Load a model from the specified directory and checkpoint file.\n",
    "    \n",
    "    Parameters:\n",
    "    exp_dir (str): The experiment directory path.\n",
    "    model (torch.nn.Module): The model instance to load the state dictionary into.\n",
    "    mpath (str): The path to the model checkpoint file.\n",
    "    \n",
    "    Returns:\n",
    "    torch.nn.Module: The loaded model with the state dictionary.\n",
    "    \n",
    "    Raises:\n",
    "    FileNotFoundError: If the model file does not exist at the specified path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(mpath):\n",
    "            raise FileNotFoundError(f\"Model file not found at {mpath}\")\n",
    "        \n",
    "        # Load the checkpoint\n",
    "        ckpt = torch.load(mpath)\n",
    "        \n",
    "        # Load the state dictionary into the model\n",
    "        model.load_state_dict(ckpt['model_state_dict'])\n",
    "        \n",
    "        print(\"Model loaded successfully.\")\n",
    "        return model\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d002a2-6172-4af9-ba0c-283dec33af27",
   "metadata": {},
   "source": [
    "### Load CNF for different upsampling factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de80705e-03b5-433b-bb19-133e8f97378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf2x = srflow.SRFlow((in_channels, height, width), args.filter_size, args.L, args.K, args.bsz, 2, args.constraint, args.nb, args.condch, args.noscale, args.noscaletest)\n",
    "cnf4x = srflow.SRFlow((in_channels, height, width), args.filter_size, args.L, args.K, args.bsz, 4, args.constraint, args.nb, args.condch, args.noscale, args.noscaletest)\n",
    "cnf8x = srflow.SRFlow((in_channels, height, width), args.filter_size, args.L, args.K, args.bsz, 8, args.constraint, args.nb, args.condch, args.noscale, args.noscaletest)\n",
    "cnf16x = srflow.SRFlow((in_channels, height, width), args.filter_size, args.L, args.K, args.bsz, 16, args.constraint, args.nb, args.condch, args.noscale, args.noscaletest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1761356-5dd2-44b7-bbf5-0966536b00b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # note: already have results for 4x\n",
    "# exp_dir = 'runs/srflow_era5-TCW_None_2024_06_22_09_00_21_2x/'\n",
    "# cnf_path = '/home/christina/Documents/clim-var-ds-cnf-own/runs/srflow_era5-TCW_None_2024_06_22_09_00_21_2x/model_checkpoints/model_epoch_0_step_2000.tar'\n",
    "# # Load the model\n",
    "# cnf2x = load_model(exp_dir, cnf2x, cnf_path).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf647d-bb6c-4d3e-a9f6-92945fece6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_dir = 'runs/srflow_era5-TCW_None_2024_06_22_09_00_21_4x/'\n",
    "# cnf_path = '/home/christina/Documents/clim-var-ds-cnf-own/runs/srflow_era5-TCW_None_2024_06_22_09_00_21_4x/model_checkpoints/model_epoch_0_step_2000.tar'\n",
    "# # Load the model\n",
    "# cnf4x = load_model(exp_dir, cnf4x, cnf_path).to(args.device)\n",
    "# args.s = 4\n",
    "# train_loader,val_loader, test_loader, args = dataloading.load_data(args)\n",
    "# metric_eval(test_loader, cnf4x, exp_dir, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2328e1c9-d3c0-4da8-9379-2031f38687e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = 'runs/srflow_era5-TCW_None_2024_06_22_09_00_20_8x/'\n",
    "cnf_path = '/home/christina/Documents/clim-var-ds-cnf-own/runs/srflow_era5-TCW_None_2024_06_22_09_00_20_8x/model_checkpoints/model_epoch_2_step_5750.tar'\n",
    "# Load the model\n",
    "cnf8x = load_model(exp_dir, cnf8x, cnf_path).to(args.device)\n",
    "args.s = 8\n",
    "train_loader, val_loader, test_loader, args = dataloading.load_data(args)\n",
    "metric_eval(test_loader, cnf8x, exp_dir, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f5099-69e7-4219-98b8-257ab3ea8624",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = 'runs/srflow_era5-TCW_None_2024_06_22_09_00_13_16x/'\n",
    "cnf_path = '/home/christina/Documents/clim-var-ds-cnf-own/runs/srflow_era5-TCW_None_2024_06_22_09_00_13_16x/model_checkpoints/model_epoch_2_step_5750.tar'\n",
    "# Load the model\n",
    "cnf16x = load_model(exp_dir, cnf16x, cnf_path).to(args.device)\n",
    "args.s = 16\n",
    "train_loader,val_loader, test_loader, args = dataloading.load_data(args)\n",
    "metric_eval(test_loader, cnf16x, exp_dir, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36857ab0-5c7a-4979-a71b-64333a140844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# especially focus on analyzing extreme value predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73b7182-a7e8-44f5-b448-e1b251e8754e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4656e27c-4d6c-4ac7-b22c-407cf1eba5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a0776-5bfe-45ce-9170-fb5c77cc30ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_std(model):\n",
    "    \"\"\"\n",
    "    For this experiment we visualize the super-resolution space for a single\n",
    "    low-resolution image and its possible HR target predictions. We visualize\n",
    "    the standard deviation of these predictions from the mean of the model.\n",
    "    \"\"\"\n",
    "    color = 'plasma'\n",
    "    savedir_viz = \"experiments/{}_{}_{}/snapshots/population_std/\".format(exp_name, modelname, args.trainset)\n",
    "    os.makedirs(savedir_viz, exist_ok=True)\n",
    "    model.eval()\n",
    "    cmap = 'viridis' if args.trainset == 'era5-TCW' else 'inferno'\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, item in enumerate(test_loader):\n",
    "\n",
    "            y = item[0].to(args.device)\n",
    "            x = item[1].to(args.device)\n",
    "\n",
    "            y_unorm = item[2].to(args.device)\n",
    "            x_unnorm = item[3].to(args.device)\n",
    "\n",
    "            mu0 = model(x)\n",
    "\n",
    "            samples = []\n",
    "            n = 20\n",
    "            sq_diff = torch.zeros_like(mu0)\n",
    "            for n in range(n):\n",
    "                mu1 = model(x)\n",
    "                samples.append(mu0)\n",
    "                sq_diff += (mu1 - mu0)**2\n",
    "\n",
    "            # compute population standard deviation\n",
    "            sigma = torch.sqrt(sq_diff / n)\n",
    "\n",
    "            # create plot\n",
    "            plt.figure()\n",
    "            plt.imshow(sigma[0,...].permute(1,2,0).cpu().numpy(), cmap=color)\n",
    "            plt.axis('off')\n",
    "            # plt.show()\n",
    "            plt.savefig(savedir_viz + '/sigma_{}.png'.format(batch_idx), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "            plt.figure()\n",
    "            plt.imshow(mu0[0,...].permute(1,2,0).cpu().numpy(), cmap=cmap)\n",
    "            plt.axis('off')\n",
    "            # plt.show()\n",
    "            plt.savefig(savedir_viz + '/mu0_{}.png'.format(batch_idx), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "            fig, (ax1, ax3, ax4, ax5, ax6, ax7) = plt.subplots(1,6)\n",
    "            # fig.suptitle('Y, Y_hat, mu, sigma')\n",
    "            ax1.imshow(y[0,...].permute(1,2,0).cpu().numpy(), cmap=cmap)\n",
    "            divider = make_axes_locatable(ax1)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            cax.set_axis_off()\n",
    "            ax1.set_title('Ground Truth', fontsize=5)\n",
    "            ax1.axis('off')\n",
    "            # ax2.imshow(mu0[0,...].permute(1,2,0).cpu().numpy(), cmap=cmap)\n",
    "            # divider = make_axes_locatable(ax2)\n",
    "            # cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            # cax.set_axis_off()\n",
    "            # ax2.set_title('Mean', fontsize=5)\n",
    "            # ax2.axis('off')\n",
    "            ax3.imshow(samples[1][0,...].permute(1,2,0).cpu().numpy(), cmap=cmap)\n",
    "            divider = make_axes_locatable(ax3)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            cax.set_axis_off()\n",
    "            ax3.set_title('Sample 1', fontsize=5)\n",
    "            ax3.axis('off')\n",
    "            ax4.imshow(samples[2][0,...].permute(1,2,0).cpu().numpy(), cmap=cmap)\n",
    "            divider = make_axes_locatable(ax4)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            cax.set_axis_off()\n",
    "            ax4.set_title('Sample 2', fontsize=5)\n",
    "            ax4.axis('off')\n",
    "            ax5.imshow(samples[2][0,...].permute(1,2,0).cpu().numpy(), cmap=cmap)\n",
    "            divider = make_axes_locatable(ax5)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            cax.set_axis_off()\n",
    "            ax5.set_title('Sample 3', fontsize=5)\n",
    "            ax5.axis('off')\n",
    "            ax6.imshow(samples[2][0,...].permute(1,2,0).cpu().numpy(), cmap=cmap)\n",
    "            divider = make_axes_locatable(ax6)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            cax.set_axis_off()\n",
    "            ax6.set_title('Sample 4', fontsize=5)\n",
    "            ax6.axis('off')\n",
    "            divider = make_axes_locatable(ax7)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            im7 = ax7.imshow(sigma[0,...].permute(1,2,0).cpu().numpy(), cmap='magma')\n",
    "            cbar = fig.colorbar(im7,cmap='magma', cax=cax)\n",
    "            cbar.ax.tick_params(labelsize=5)\n",
    "            ax7.set_title('Std. Dev.', fontsize=5)\n",
    "            ax7.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(savedir_viz + '/std_multiplot_{}.png'.format(batch_idx), dpi=300, bbox_inches='tight')\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d17b1a-4477-4795-8c33-a2083b6ebb6a",
   "metadata": {},
   "source": [
    "### Scatter Plot (Predicted vs Target)\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf742aa2-ecdc-460e-bf97-82a6075b7b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b11941a-43fc-4b2a-b283-8e1a3d58e592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b95711-c723-42a2-b197-2769d97f2017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fc2171-68d2-49a3-adb2-d4a53f529af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6607475d-1f22-4727-83ca-74ae1eb29b3d",
   "metadata": {},
   "source": [
    "### Cumulative Distribution of  Residual Errors\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df4f9f0-4531-4d78-ae7b-ae5eb7a93515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e4f573-4a04-44ff-a477-be2ac8b9f0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6962eb3c-5c63-40e8-9385-88fee3493ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6af5eb-8b42-45cc-ac6c-bdd10ee86af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2208b98-353a-4595-811b-38af176c3daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05c832a-84fd-4e17-8dd5-bc32890cd0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32abac29",
   "metadata": {},
   "source": [
    "### Experiments: Constraint placement\n",
    "Comparing model with no constraint vs. multiplicative constraint at output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0405b1a7-1b9a-488e-8ec7-f9a8edce901b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd646367-8e67-44c2-914d-a9a3a25b4417",
   "metadata": {},
   "source": [
    "### Load CNF with constraints 4x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a5d71-49df-4597-92f6-8b248097bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_4x_add = srflow.SRFlow((in_channels, height, width), args.filter_size, args.L, args.K, args.bsz, 4, 'add', args.nb, args.condch, args.noscale, args.noscaletest)\n",
    "cnf_4x_mult = srflow.SRFlow((in_channels, height, width), args.filter_size, args.L, args.K, args.bsz, 4, 'mult', args.nb, args.condch, args.noscale, args.noscaletest)\n",
    "cnf_4x_scadd = srflow.SRFlow((in_channels, height, width), args.filter_size, args.L, args.K, args.bsz, 4, 'scadd', args.nb, args.condch, args.noscale, args.noscaletest)\n",
    "cnf_4x_softmax = srflow.SRFlow((in_channels, height, width), args.filter_size, args.L, args.K, args.bsz, 4, 'softmax', args.nb, args.condch, args.noscale, args.noscaletest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377ed938-5fb1-45fe-819b-33f022acd41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vanilla conditional flow 4x\n",
    "exp_dir_add_4x = 'runs/srflow_era5-TCW_None_2024_06_22_09_00_21_4x/'\n",
    "exp_dir_mult_4x = 'runs/srflow_era5-TCW_None_2024_06_22_09_00_21_4x/'\n",
    "exp_dir_scadd_4x = 'runs/srflow_era5-TCW_None_2024_06_22_09_00_21_4x/'\n",
    "exp_dir_softmax_4x = 'runs/srflow_era5-TCW_None_2024_06_22_09_00_21_4x/'\n",
    "\n",
    "cnf_4x_add_path = '/home/christina/Documents/clim-var-ds-cnf-own/runs/srflow_era5-TCW_None_2024_06_22_09_00_21_4x/model_checkpoints/model_epoch_0_step_2000.tar'\n",
    "cnf_4x_mult_path = '/home/christina/Documents/clim-var-ds-cnf-own/runs/srflow_era5-TCW_None_2024_06_22_09_00_21_4x/model_checkpoints/model_epoch_0_step_2000.tar'\n",
    "cnf_4x_scadd_path = '/home/christina/Documents/clim-var-ds-cnf-own/runs/srflow_era5-TCW_None_2024_06_22_09_00_21_4x/model_checkpoints/model_epoch_0_step_2000.tar'\n",
    "cnf_4x_softmax_path = '/home/christina/Documents/clim-var-ds-cnf-own/runs/srflow_era5-TCW_None_2024_06_22_09_00_21_4x/model_checkpoints/model_epoch_0_step_2000.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e566603a-ce34-4d54-911d-a9cae9e766fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade9a6c3-0ead-4f14-966e-67667b90c826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434bd06b-6d76-4940-849d-83cac155a2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe5bdf-1cb5-4f99-91f2-5bb95b4c3326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0040259-9f91-427e-992a-d44ffb31b18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1f4fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2x watercontent mul with constraint at the end \n",
    "modelname = 'model_epoch_0_step_1000'\n",
    "modelpath = '/home/mila/c/christina.winkler/clim-var-ds-cnf/runs/srflow_era5-TCW_mul_ constr_in_end__2024_06_03_17_35_33_2x/model_checkpoints/{}.tar'.format(modelname)\n",
    "\n",
    "model = srflow.SRFlow((in_channels, args.height, args.width), args.filter_size, args.L, args.K,\n",
    "                        args.bsz, args.s, 'mul', args.nb, args.condch, args.noscale, args.noscaletest)\n",
    "\n",
    "params = sum(x.numel() for x in model.parameters() if x.requires_grad)\n",
    "print('Nr of Trainable Params {}:  '.format(args.device), params)\n",
    "model = model.to(args.device)\n",
    "exp_name = \"flow-{}-level-{}-k--constraint-{}\".format(args.L, args.K, 'mul')\n",
    "print(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fde121",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(modelpath, map_location='cuda:0')\n",
    "model.load_state_dict(ckpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbba4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2x watercontent with constraint after RDDB upscaling layers\n",
    "# modelname = 'model_epoch_0_step_1000'\n",
    "# modelpath = '/home/mila/c/christina.winkler/clim-var-ds-cnf/runs/srflow_era5-TCW_mul_ constr_in_end__2024_06_03_17_35_33_2x/model_checkpoints/{}.tar'.format(modelname)\n",
    "\n",
    "# model = srflow.SRFlow((in_channels, height, width), args.filter_size, args.L, args.K,\n",
    "#                        args.bsz, args.s, 'mul', args.nb, args.condch, args.noscale, args.noscaletest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e5c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = torch.load(modelpath, map_location='cuda:0')\n",
    "# model.load_state_dict(ckpt['model_state_dict'])\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db0d7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2x watercontent with no constraint\n",
    "# modelname = 'model_epoch_0_step_1000'\n",
    "# modelpath = '/home/mila/c/christina.winkler/clim-var-ds-cnf/runs/srflow_era5-TCW_mul_ constr_in_end__2024_06_03_17_35_33_2x/model_checkpoints/{}.tar'.format(modelname)\n",
    "\n",
    "# model = srflow.SRFlow((in_channels, height, width), args.filter_size, args.L, args.K,\n",
    "#                        args.bsz, args.s, 'mul', args.nb, args.condch, args.noscale, args.noscaletest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = torch.load(modelpath, map_location='cuda:0')\n",
    "# model.load_state_dict(ckpt['model_state_dict'])\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ac075a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3d3d3af",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a749c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b015c880",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d125f64",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd755b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stflow",
   "language": "python",
   "name": "stflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
